{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJH-mcs5q0Bu"
      },
      "outputs": [],
      "source": [
        "!pip install \"git+https://github.com/nixtla/neuralforecast.git@main\"\n",
        "!pip install darts"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import packages\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from neuralforecast import NeuralForecast\n",
        "from neuralforecast.losses.pytorch import MQLoss\n",
        "\n",
        "from datetime import date\n",
        "from darts import TimeSeries, concatenate\n",
        "from darts.dataprocessing.transformers import Scaler\n",
        "from darts.utils.statistics import check_seasonality, extract_trend_and_seasonality\n",
        "from darts.metrics import mape, rmse, mae, smape\n",
        "from darts.utils.timeseries_generation import datetime_attribute_timeseries, holidays_timeseries\n",
        "from darts.utils.likelihood_models import QuantileRegression"
      ],
      "metadata": {
        "id": "CrV_rSQeq6l8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_df = pd.read_csv('Y_df.csv')\n",
        "\n",
        "Y_df['datetime'] = pd.to_datetime(Y_df['ds'])\n",
        "Y_df.drop('ds', axis=1, inplace=True)\n",
        "Y_df.set_index('datetime', inplace=True)\n",
        "series = TimeSeries.from_series(Y_df)\n",
        "\n",
        "series = series.add_datetime_attribute('hour')\n",
        "series = series.add_datetime_attribute('dayofweek')\n",
        "series = series.add_datetime_attribute('month')\n",
        "series = series.add_datetime_attribute('quarter')\n",
        "series = series.add_datetime_attribute('day')\n",
        "series = series.add_datetime_attribute('year')\n",
        "series = series.add_holidays(country_code='ITA')\n",
        "\n",
        "Y_df = TimeSeries.pd_dataframe(series).reset_index()\n",
        "Y_df.rename({'datetime': 'ds'}, axis=1, inplace=True)\n",
        "Y_df"
      ],
      "metadata": {
        "id": "Ba-55G96q6jn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "Y_df.reset_index(inplace=True)\n",
        "def is_bridge_day(day, holiday, dayofweek):\n",
        "    if holiday == 1:\n",
        "        return 7\n",
        "    elif dayofweek in [5,6] and holiday == 0:\n",
        "        return dayofweek\n",
        "    elif dayofweek == 0 and holiday == 0 and Y_df.iloc[day+24]['holidays'] == 1:\n",
        "        return 8\n",
        "    elif dayofweek == 4 and holiday == 0 and Y_df.iloc[day-24]['holidays'] == 1:\n",
        "        return 8\n",
        "    else:\n",
        "        return dayofweek\n",
        "\n",
        "Y_df['day_of_week'] = np.vectorize(is_bridge_day)(Y_df.index, Y_df['holidays'], Y_df['dayofweek'])\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "\n",
        "calendar = ['hour', 'day', 'day_of_week', 'month', 'year']\n",
        "for cal in calendar:\n",
        "  Y_df[cal] = encoder.fit_transform(Y_df[cal]).astype(np.int32)\n",
        "Y_df"
      ],
      "metadata": {
        "id": "VlQoZwkGq6hQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_size  = 190*24\n",
        "test_size = 190*24\n",
        "Y_train_df = Y_df.iloc[:-test_size, :]\n",
        "Y_test_df = Y_df.iloc[-test_size:, :]\n",
        "Y_val_df = Y_train_df.iloc[-val_size:, :]"
      ],
      "metadata": {
        "id": "-HOXZGxFq6es"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the plot\n",
        "import matplotlib\n",
        "matplotlib.rc_file_defaults()\n",
        "\n",
        "# Create subplots with two rows\n",
        "fig, axs = plt.subplots(3, 1, figsize=(15, 12), sharex=True)\n",
        "\n",
        "# Plot the load data in the first subplot\n",
        "axs[0].plot(Y_train_df['ds'], Y_train_df['y'], color='steelblue', label='Electricity Price')\n",
        "axs[0].plot(Y_val_df['ds'], Y_val_df['y'], color='steelblue')\n",
        "axs[0].plot(Y_test_df['ds'], Y_test_df['y'], color='steelblue')\n",
        "axs[0].set_ylabel('Spot Price (€/MWh)', fontsize=12)\n",
        "axs[0].legend(loc = \"upper left\")\n",
        "legend = axs[0].get_legend()\n",
        "\n",
        "# Set the font size of the legend\n",
        "legend.get_frame().set_facecolor('white')  # Optional: Set legend background color\n",
        "legend.get_frame().set_linewidth(0.5)  # Optional: Set legend frame linewidth\n",
        "for text in legend.get_texts():\n",
        "    text.set_fontsize(12)  # Set the font size\n",
        "\n",
        "\n",
        "# Plot the temperature data in the second subplot\n",
        "axs[1].plot(Y_train_df['ds'], Y_train_df['psvda'], color='seagreen', label='PSVDA')\n",
        "axs[1].plot(Y_val_df['ds'], Y_val_df['psvda'], color='seagreen')\n",
        "axs[1].plot(Y_test_df['ds'], Y_test_df['psvda'], color='seagreen')\n",
        "axs[1].set_ylabel('PSV day-ahead (€/MWh)', fontsize=12)\n",
        "axs[1].legend(loc=\"upper left\")\n",
        "# Get the current legend\n",
        "legend_1 = axs[1].get_legend()\n",
        "\n",
        "# Set the font size of the legend\n",
        "legend_1.get_frame().set_facecolor('white')  # Optional: Set legend background color\n",
        "legend_1.get_frame().set_linewidth(0.5)  # Optional: Set legend frame linewidth\n",
        "for text in legend_1.get_texts():\n",
        "    text.set_fontsize(12)  # Set the font size\n",
        "\n",
        "# Plot the temperature data in the second subplot\n",
        "axs[2].plot(Y_train_df['ds'], Y_train_df['load forecast'], color='lightcoral', label='Load Forecast')\n",
        "axs[2].plot(Y_val_df['ds'], Y_val_df['load forecast'], color='lightcoral')\n",
        "axs[2].plot(Y_test_df['ds'], Y_test_df['load forecast'], color='lightcoral')\n",
        "axs[2].set_ylabel('Load (MW)', fontsize=12)\n",
        "axs[2].legend(loc=\"upper left\")\n",
        "# Get the current legend\n",
        "legend_2 = axs[2].get_legend()\n",
        "\n",
        "# Set the font size of the legend\n",
        "legend_2.get_frame().set_facecolor('white')  # Optional: Set legend background color\n",
        "legend_2.get_frame().set_linewidth(0.5)  # Optional: Set legend frame linewidth\n",
        "for text in legend_2.get_texts():\n",
        "    text.set_fontsize(12)  # Set the font size\n",
        "\n",
        "\n",
        "# Add annotations for the splits\n",
        "axs[0].annotate('Training', xy=(Y_train_df['ds'].mean(), Y_train_df['y'].max()), xytext=(0, 20),\n",
        "             xycoords='data', textcoords='offset points', fontsize=15, ha='center')\n",
        "axs[0].annotate('Validation', xy=(Y_val_df['ds'].mean(), Y_train_df['y'].max()), xytext=(0, 20),\n",
        "             xycoords='data', textcoords='offset points', fontsize=15, ha='center')\n",
        "axs[0].annotate('Test', xy=(Y_test_df['ds'].mean(), Y_train_df['y'].max()), xytext=(0, 20),\n",
        "             xycoords='data', textcoords='offset points', fontsize=15, ha='center')\n",
        "\n",
        "# Add dashed lines for the splits\n",
        "axs[0].axvline(Y_val_df['ds'].iloc[0], color='k', linestyle='--')\n",
        "axs[0].axvline(Y_val_df['ds'].iloc[-1], color='k', linestyle='--')\n",
        "axs[1].axvline(Y_val_df['ds'].iloc[0], color='k', linestyle='--')  # Shared vertical line\n",
        "axs[1].axvline(Y_val_df['ds'].iloc[-1], color='k', linestyle='--')\n",
        "axs[2].axvline(Y_val_df['ds'].iloc[0], color='k', linestyle='--')  # Shared vertical line\n",
        "axs[2].axvline(Y_val_df['ds'].iloc[-1], color='k', linestyle='--')\n",
        "\n",
        "plt.xlabel('Datetime', fontsize=12)\n",
        "#plt.suptitle('Electricity Demand', fontsize=15)\n",
        "plt.subplots_adjust(hspace=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0TZDaM_urvLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create a copy of Y_train_df\n",
        "df_train = Y_train_df.copy()\n",
        "\n",
        "# Map quarter labels to the \"Quarter\" column\n",
        "quarter_labels = {1: 'Q1', 2: 'Q2', 3: 'Q3', 4: 'Q4'}\n",
        "df_train['Quarter'] = df_train['quarter'].map(quarter_labels)\n",
        "\n",
        "# Create the plot\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.lineplot(data=df_train, x='hour', y='y', hue='Quarter', palette=palette, linewidth=3)\n",
        "\n",
        "# Remove the spines\n",
        "sns.despine()\n",
        "\n",
        "# Set the title\n",
        "plt.ylabel('Price (€/MWh)')\n",
        "\n",
        "# Change the legend labels\n",
        "legend = plt.legend(fontsize='large')\n",
        "for line, label in zip(legend.get_lines(), quarter_labels.values()):\n",
        "    line.set_linewidth(3.0)\n",
        "    line.set_label(label)\n",
        "plt.grid(False)\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Ji3CkoFarvnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_df.set_index('ds', inplace=True)"
      ],
      "metadata": {
        "id": "1IL3ltT0r3N8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_4 = Y_df[['y', 'hour', 'dayofweek', 'month', 'holidays', 'day', 'year', 'psvda', 'load forecast']]\n",
        "\n",
        "# Suppose you have a DataFrame called 'df' containing the relevant data including 'hour_profile' column\n",
        "\n",
        "# Create past regressor columns\n",
        "num_past_periods = 168  # Number of past observed periods\n",
        "\n",
        "# past gas\n",
        "df_4[f'psvda_t-{24}'] = Y_df['psvda'].shift(24)\n",
        "\n",
        "# target\n",
        "for i in range(1, 24):\n",
        "    df_4[f'y_t+{i}'] = Y_df['y'].shift(-i)\n",
        "\n",
        "# future load\n",
        "for i in range(0, 24):\n",
        "    df_4[f'load_forecast_t+{i}'] = Y_df['load forecast'].shift(-i)\n",
        "\n",
        "for i in range(1, 169):\n",
        "    df_4[f'y_t-{i}'] = Y_df['y'].shift(i)\n",
        "\n",
        "df_4.rename({'y': 'y_t+0'}, axis=1, inplace=True)\n",
        "# Remove rows with NaN values due to shifting\n",
        "df_4 = df_4.dropna()\n",
        "df_4['weekday-hour'] = df_4['dayofweek']*df_4['hour']\n",
        "df_4['month-hour'] = df_4['month']*df_4['hour']\n",
        "df_4['year'] = [x[:4] for x in df_4.index.astype('str')]\n",
        "df_4[['hour', 'dayofweek', 'month', 'holidays', 'day', 'year', 'weekday-hour', 'month-hour']] = df_4[['hour', 'dayofweek', 'month', 'holidays', 'day', 'year', 'weekday-hour', 'month-hour']].astype(np.int32)\n",
        "df_4"
      ],
      "metadata": {
        "id": "JRDmkCGOsEML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "df_4.reset_index(inplace=True)\n",
        "def is_bridge_day(day, holiday, dayofweek):\n",
        "    if holiday == 1:\n",
        "        return 7\n",
        "    elif dayofweek in [5,6] and holiday == 0:\n",
        "        return dayofweek\n",
        "    elif dayofweek == 0 and holiday == 0 and df_4.iloc[day+24]['holidays'] == 1:\n",
        "        return 8\n",
        "    elif dayofweek == 4 and holiday == 0 and df_4.iloc[day-24]['holidays'] == 1:\n",
        "        return 8\n",
        "    else:\n",
        "        return dayofweek\n",
        "\n",
        "df_4['day_of_week'] = np.vectorize(is_bridge_day)(df_4.index, df_4['holidays'], df_4['dayofweek'])"
      ],
      "metadata": {
        "id": "T7L10ZvbsKf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exog_past = [f'y_t-{i}' for i in range(1, 169)]\n",
        "exog_past.append('psvda_t-24')\n",
        "\n",
        "exog_fut = [f'load_forecast_t+{i}' for i in range(0, 24)]\n",
        "exog = exog_past + exog_fut\n",
        "\n",
        "target = [f'y_t+{i}' for i in range(24)]\n",
        "\n",
        "calendar = ['hour', 'day_of_week', 'month', 'day', 'year']\n",
        "df_4.set_index('ds', inplace=True)"
      ],
      "metadata": {
        "id": "6BJlUI3SsOkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "import numpy as np\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "\n",
        "for cal in calendar:\n",
        "  df_4[cal] = encoder.fit_transform(df_4[cal]).astype(np.int32)"
      ],
      "metadata": {
        "id": "We2DvTqosQEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming your dataset is stored in a DataFrame called df_4\n",
        "\n",
        "# Determine the indices for splitting the dataset\n",
        "test_index = len(df_4) - 190*24\n",
        "validation_index = test_index - 190*24\n",
        "\n",
        "# Split the dataset into training, validation, and test sets\n",
        "Y_train_df = df_4.iloc[:validation_index]\n",
        "Y_val_df = df_4.iloc[validation_index:test_index]\n",
        "Y_test_df = df_4.iloc[test_index:]\n",
        "\n",
        "plt.plot(Y_train_df.index, Y_train_df['y_t+0'])\n",
        "plt.plot(Y_val_df.index, Y_val_df['y_t+0'])\n",
        "plt.plot(Y_test_df.index, Y_test_df['y_t+0'])"
      ],
      "metadata": {
        "id": "l8MsQQpisSQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler_1 = MinMaxScaler()\n",
        "scaler_2 = MinMaxScaler()\n",
        "variables = exog + calendar\n",
        "\n",
        "scaler_1.fit(Y_train_df[exog])\n",
        "scaler_2.fit(Y_train_df[target])\n",
        "\n",
        "# training set scaled\n",
        "Y_train_scaled_exog = pd.DataFrame(scaler_1.transform(Y_train_df[exog]), columns=Y_train_df[exog].columns, index=Y_train_df.index)\n",
        "Y_train_scaled_df = pd.concat([Y_train_scaled_exog, Y_train_df[calendar]], axis=1)\n",
        "Y_train_scaled_y = pd.DataFrame(scaler_2.transform(Y_train_df[target]), columns=Y_train_df[target].columns)\n",
        "# test set scaled\n",
        "Y_test_scaled_exog = pd.DataFrame(scaler_1.transform(Y_test_df[exog]), columns=Y_test_df[exog].columns, index=Y_test_df.index)\n",
        "Y_test_scaled_df = pd.concat([Y_test_scaled_exog, Y_test_df[calendar]], axis=1)\n",
        "Y_test_scaled_y = pd.DataFrame(scaler_2.transform(Y_test_df[target]), columns=Y_test_df[target].columns)\n",
        "\n",
        "# validation set scaled\n",
        "Y_val_scaled_exog = pd.DataFrame(scaler_1.transform(Y_val_df[exog]), columns=Y_val_df[exog].columns, index=Y_val_df.index)\n",
        "Y_val_scaled_df = pd.concat([Y_val_scaled_exog, Y_val_df[calendar]], axis=1)\n",
        "Y_val_scaled_y = pd.DataFrame(scaler_2.transform(Y_val_df[target]), columns=Y_val_df[target].columns)\n",
        "\n",
        "\n",
        "# train + validation set scaled\n",
        "Y_trainval_scaled_df = pd.concat([Y_train_scaled_df, Y_val_scaled_df], axis=0)\n",
        "Y_trainval_scaled_y = pd.concat([Y_train_scaled_y, Y_val_scaled_y], axis=0)"
      ],
      "metadata": {
        "id": "AabPESWMtFfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "# Sample dat\n",
        "# Assume you have a DataFrame called 'df' containing the relevant data including calendar variables and percentage deviations.\n",
        "\n",
        "# Define input shapes\n",
        "num_calendar_features = len(calendar)  # Number of calendar features: hour, day_of_month, day_of_week, holiday, month, quarter\n",
        "num_exog_features = len(exog)  # Number of past observed percentage deviations to consider as input\n",
        "\n",
        "# Prepare train input data\n",
        "calendar_features = Y_train_scaled_df[calendar].values\n",
        "\n",
        "past_regressors = Y_train_scaled_df[exog].values\n",
        "\n",
        "target_variable = Y_train_scaled_y[target].values\n",
        "\n",
        "# Prepare val input data\n",
        "calendar_features_val = Y_val_scaled_df[calendar].values\n",
        "\n",
        "past_regressors_val = Y_val_scaled_df[exog].values\n",
        "\n",
        "target_variable_val = Y_val_scaled_y[target].values\n",
        "\n",
        "#Prepare test input data\n",
        "test_calendar_features = Y_test_scaled_df[calendar].values\n",
        "test_past_regressors = Y_test_scaled_df[exog].values\n",
        "\n",
        "# training + validation input data\n",
        "trainval_calendar_features = Y_trainval_scaled_df[calendar].values\n",
        "\n",
        "trainval_past_regressors = Y_trainval_scaled_df[exog].values\n",
        "\n",
        "trainval_target = Y_trainval_scaled_y[target].values"
      ],
      "metadata": {
        "id": "rYMpEEFotIds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Concatenate, Dense, Embedding, Flatten, Input, BatchNormalization, Dropout\n",
        "from keras.layers import TimeDistributed, Reshape\n",
        "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "def quantile_loss(q, y_true, y_pred):\n",
        "    error = (y_true - y_pred)\n",
        "    return tf.reduce_mean(tf.maximum(q * error, (q - 1) * error), axis=-1)"
      ],
      "metadata": {
        "id": "Ds5hKw87tKXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define input layers\n",
        "hour_input = Input(shape=(1,), name='hour_input')\n",
        "dow_input = Input(shape=(1,), name='dow_input')\n",
        "month_input = Input(shape=(1,), name='month_input')\n",
        "dom_input = Input(shape=(1,), name='dom_input')\n",
        "year_input = Input(shape=(1,), name='year_input')\n",
        "weho_input = Input(shape=(1,), name='week_hour_input')\n",
        "moho_input = Input(shape=(1,), name='month_hour_input')\n",
        "\n",
        "past_regressors_input = Input(shape=(num_exog_features,), name='past_regressors_input')\n",
        "\n",
        "# Embedding layer for \"hour\" variable\n",
        "hour_embedding_dim = 8\n",
        "embedded_hour = Embedding(input_dim=24, output_dim=hour_embedding_dim, input_length=1, name='embedding_hour')(hour_input)\n",
        "flatten_hour = Flatten()(embedded_hour)\n",
        "\n",
        "# Embedding layer for \"day of week\" variable\n",
        "dow_embedding_dim = 3\n",
        "embedded_dow = Embedding(input_dim=9, output_dim=dow_embedding_dim, input_length=1, name='embedding_dow')(dow_input)\n",
        "flatten_dow = Flatten()(embedded_dow)\n",
        "\n",
        "# Embedding layer for \"month\" variable\n",
        "month_embedding_dim = 4\n",
        "embedded_month = Embedding(input_dim=12, output_dim=month_embedding_dim, input_length=1, name='embedding_month')(month_input)\n",
        "flatten_month = Flatten()(embedded_month)\n",
        "\n",
        "# Embedding layer for \"day of month\" variable\n",
        "dom_embedding_dim = 10\n",
        "embedded_dom = Embedding(input_dim=31, output_dim=dom_embedding_dim, input_length=1, name='embedding_dom')(dom_input)\n",
        "flatten_dom = Flatten()(embedded_dom)\n",
        "\n",
        "# Embedding layer for \"year\" variable\n",
        "year_embedding_dim = 3\n",
        "embedded_year = Embedding(input_dim=6, output_dim=year_embedding_dim, input_length=1, name='embedding_year')(year_input)\n",
        "flatten_year = Flatten()(embedded_year)\n",
        "\n",
        "\n",
        "# Concatenate the embedding layers and past_deviations_input\n",
        "merged_inputs = Concatenate()([flatten_hour,\n",
        "                               flatten_dow, flatten_month,\n",
        "                               flatten_dom, flatten_year,\n",
        "                               past_regressors_input])\n",
        "\n",
        "\n",
        "def my_probmodel(neurons, activation):\n",
        "\n",
        "  # Add dense layers\n",
        "  h = Dense(neurons, activation=activation)(merged_inputs)\n",
        "  h = Dense(neurons, activation=activation)(h)\n",
        "\n",
        "  # Create separate output neurons for each time step\n",
        "  outputs = []\n",
        "  for i in range(24):\n",
        "      output = Dense(1, activation='relu', name=f'time_step_{i+1}')(h)\n",
        "      outputs.append(output)\n",
        "\n",
        "  # Create the model\n",
        "  model = keras.Model(inputs=[hour_input,\n",
        "                            dow_input, month_input,\n",
        "                            dom_input, year_input,\n",
        "                            past_regressors_input],\n",
        "                            outputs=outputs)\n",
        "  return model\n"
      ],
      "metadata": {
        "id": "2SkM89nytKzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x=my_probmodel_dist_gaus(160, 'relu')\n",
        "x.summary()\n",
        "from keras.utils import plot_model\n",
        "plot_model(x, rankdir='LR')"
      ],
      "metadata": {
        "id": "-aksN0WGtpGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from keras.callbacks import TensorBoard\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from tensorflow.keras import regularizers\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "\n",
        "\n",
        "log_dir='logs'\n",
        "# Create a dictionary to map embedding layer names to their metadata files\n",
        "embeddings_metadata = {\n",
        "    'embedding_hour': 'metadata/metadata_hour.tsv',\n",
        "    'embedding_dow': 'metadata/metadata_dow.tsv',\n",
        "    'embedding_month': 'metadata/metadata_month.tsv',\n",
        "    'embedding_dom': 'metadata/metadata_dom.tsv',\n",
        "    'embedding_year': 'metadata/metadata_year.tsv',\n",
        "}\n",
        "\n",
        "# Initialize the TensorBoard callback with the embeddings metadata\n",
        "tensorboard_callback = TensorBoard(log_dir=log_dir, embeddings_freq=1, embeddings_metadata=embeddings_metadata)"
      ],
      "metadata": {
        "id": "fAEsOYqFtsDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainval_calendar_features = trainval_calendar_features.astype(np.float32)\n",
        "trainval_past_regressors = trainval_past_regressors.astype(np.float32)\n",
        "trainval_target = trainval_target.astype(np.float32)"
      ],
      "metadata": {
        "id": "GDevJRkRtuSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tuner"
      ],
      "metadata": {
        "id": "WkJEWZQftuPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers.normalization import batch_normalization\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from kerastuner.tuners import RandomSearch\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "# Define the hyperparameter search space\n",
        "def build_model(hp):\n",
        "  # Define the input layers\n",
        "    hour_input = Input(shape=(1,), name='hour')\n",
        "    dow_input = Input(shape=(1,), name='day_of_week')\n",
        "    month_input = Input(shape=(1,), name='month')\n",
        "    dom_input = Input(shape=(1,), name='day_of_month')\n",
        "    year_input = Input(shape=(1,), name='year')\n",
        "\n",
        "\n",
        "    past_regressors_input = Input(shape=(past_regressors.shape[1],), name='past_regressors')\n",
        "\n",
        "\n",
        "    # Embedding layer for \"hour\" variable\n",
        "    hour_embedding_dim = hp.Int('hour_embedding_dim', min_value=4, max_value=10)\n",
        "    embedded_hour = Embedding(input_dim=24, output_dim=hour_embedding_dim, input_length=1, name='embedding_hour')(hour_input)\n",
        "    flatten_hour = Flatten()(embedded_hour)\n",
        "\n",
        "    # Embedding layer for \"day of week\" variable\n",
        "    dow_embedding_dim = hp.Int('dow_embedding_dim', min_value=3, max_value=5)\n",
        "    embedded_dow = Embedding(input_dim=9, output_dim=dow_embedding_dim, input_length=1, name='embedding_dow')(dow_input)\n",
        "    flatten_dow = Flatten()(embedded_dow)\n",
        "\n",
        "    # Embedding layer for \"month\" variable\n",
        "    month_embedding_dim = hp.Int('month_embedding_dim', min_value=3, max_value=6)\n",
        "    embedded_month = Embedding(input_dim=12, output_dim=month_embedding_dim, input_length=1, name='embedding_month')(month_input)\n",
        "    flatten_month = Flatten()(embedded_month)\n",
        "\n",
        "    # Embedding layer for \"day of month\" variable\n",
        "    dom_embedding_dim = hp.Int('dom_embedding_dim', min_value=6, max_value=14)\n",
        "    embedded_dom = Embedding(input_dim=31, output_dim=dom_embedding_dim, input_length=1, name='embedding_dom')(dom_input)\n",
        "    flatten_dom = Flatten()(embedded_dom)\n",
        "\n",
        "    # Embedding layer for \"year\" variable\n",
        "    year_embedding_dim = hp.Int('year_embedding_dim',  min_value=2, max_value=4)\n",
        "    embedded_year = Embedding(input_dim=6, output_dim=year_embedding_dim, input_length=1, name='embedding_year')(year_input)\n",
        "    flatten_year = Flatten()(embedded_year)\n",
        "\n",
        "\n",
        "    # Concatenate the embedding layers and past_deviations_input\n",
        "    merged_inputs = Concatenate()([flatten_hour,\n",
        "                                  flatten_dow, flatten_month,\n",
        "                                  flatten_dom, flatten_year,\n",
        "                                  past_regressors_input])\n",
        "\n",
        "    quantiles = [0.1, 0.5, 0.9]  # Example quantiles\n",
        "    num_models=len(quantiles)\n",
        "    predictions = [] # store predicitons\n",
        "    model = my_probmodel_dist_gaus(160, 'relu')\n",
        "\n",
        "    # Define the hidden layers\n",
        "    units_1 = hp.Int('units_1', min_value=32, max_value=512, step=32)\n",
        "    units_2 = hp.Int('units_2', min_value=32, max_value=512, step=32)\n",
        "\n",
        "    hidden_1 = Dense(units=160, activation='relu')(merged_inputs)\n",
        "    dropout = hp.Choice('dropout', values=[True, False])\n",
        "    if dropout == True:\n",
        "        dropout_layer = Dropout(rate=hp.Choice('dropout_value', values=[0.0, 0.2, 0.4]))(hidden_1)\n",
        "    hidden_2 = Dense(units=160, activation='relu')(hidden_1)\n",
        "    else:\n",
        "        hidden_2 = Dense(units=units_1, activation='relu')(hidden_1)\n",
        "    # Create separate output neurons for each time step\n",
        "    #outputs = []\n",
        "    for i in range(24):\n",
        "        output = Dense(1, activation='linear', name=f'time_step_{i+1}')(hidden_2)\n",
        "        outputs.append(output)\n",
        "\n",
        "\n",
        "    # Create the model\n",
        "    model = keras.Model(inputs=[hour_input,\n",
        "                                  dow_input, month_input,\n",
        "                                  dom_input, year_input,\n",
        "                                  past_regressors_input],\n",
        "                                  outputs=output)\n",
        "    # Compile the model with negative log likelihood as the loss function\n",
        "    model.compile(optimizer=Adam(),  loss=lambda y_true, y_pred: negative_log_likelihood(y_true, y_pred))\n",
        "\n",
        "\n",
        "    return model\n",
        "\n",
        "# Instantiate the tuner\n",
        "tuner = RandomSearch(build_model,\n",
        "                     objective='val_loss',\n",
        "                     max_trials=10,\n",
        "                     executions_per_trial=1,\n",
        "                     directory='my_dir',\n",
        "                     project_name='my_proj',\n",
        "                     overwrite = True)\n",
        "\n",
        "# Train the tuner\n",
        "tuner.search(x=[calendar_features[:, 0], calendar_features[:, 1], calendar_features[:, 2],\n",
        "                        calendar_features[:, 3], calendar_features[:, 4],\n",
        "                       past_regressors],\n",
        "                    y=[target_variable[:, i] for i in range(24)],\n",
        "\n",
        "              validation_data=([calendar_features_val[:, 0], calendar_features_val[:, 1], calendar_features_val[:, 2],\n",
        "                       calendar_features_val[:, 3], calendar_features_val[:, 4],\n",
        "                       past_regressors_val],\n",
        "                    [target_variable_val[:, i] for i in range(24)]),\n",
        "\n",
        "              epochs=10, batch_size=32)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "best_hps"
      ],
      "metadata": {
        "id": "t1AOS6JotuMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from keras.callbacks import TensorBoard\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "log_dir='logs'\n",
        "# Create a dictionary to map embedding layer names to their metadata files\n",
        "embeddings_metadata = {\n",
        "    'embedding_hour': 'metadata/metadata_hour.tsv',\n",
        "    'embedding_dow': 'metadata/metadata_dow.tsv',\n",
        "    'embedding_month': 'metadata/metadata_month.tsv',\n",
        "    'embedding_dom': 'metadata/metadata_dom.tsv',\n",
        "    'embedding_year': 'metadata/metadata_year.tsv',\n",
        "}\n",
        "\n",
        "# Initialize the TensorBoard callback with the embeddings metadata\n",
        "tensorboard_callback = TensorBoard(log_dir=log_dir, embeddings_freq=1, embeddings_metadata=embeddings_metadata)\n",
        "\n",
        "# Define the input layers\n",
        "hour_input = Input(shape=(1,), name='hour')\n",
        "dow_input = Input(shape=(1,), name='day_of_week')\n",
        "month_input = Input(shape=(1,), name='month')\n",
        "dom_input = Input(shape=(1,), name='day_of_month')\n",
        "year_input = Input(shape=(1,), name='year')\n",
        "\n",
        "\n",
        "past_regressors_input = Input(shape=(past_regressors.shape[1],), name='past_regressors')\n",
        "\n",
        "# Embedding layer for \"hour\" variable\n",
        "hour_embedding_dim = best_hps.values['hour_embedding_dim']\n",
        "embedded_hour = Embedding(input_dim=24, output_dim=hour_embedding_dim, input_length=1, name='embedding_hour')(hour_input)\n",
        "flatten_hour = Flatten()(embedded_hour)\n",
        "\n",
        "# Embedding layer for \"day of week\" variable\n",
        "dow_embedding_dim = best_hps.values['dow_embedding_dim']\n",
        "embedded_dow = Embedding(input_dim=9, output_dim=dow_embedding_dim, input_length=1, name='embedding_dow')(dow_input)\n",
        "flatten_dow = Flatten()(embedded_dow)\n",
        "\n",
        "# Embedding layer for \"month\" variable\n",
        "month_embedding_dim = best_hps.values['month_embedding_dim']\n",
        "embedded_month = Embedding(input_dim=12, output_dim=month_embedding_dim, input_length=1, name='embedding_month')(month_input)\n",
        "flatten_month = Flatten()(embedded_month)\n",
        "\n",
        "# Embedding layer for \"day of month\" variable\n",
        "dom_embedding_dim = best_hps.values['dom_embedding_dim']\n",
        "embedded_dom = Embedding(input_dim=31, output_dim=dom_embedding_dim, input_length=1, name='embedding_dom')(dom_input)\n",
        "flatten_dom = Flatten()(embedded_dom)\n",
        "\n",
        "# Embedding layer for \"year\" variable\n",
        "year_embedding_dim = best_hps.values['year_embedding_dim']\n",
        "embedded_year = Embedding(input_dim=6, output_dim=year_embedding_dim, input_length=1, name='embedding_year')(year_input)\n",
        "flatten_year = Flatten()(embedded_year)\n",
        "\n",
        "\n",
        "\n",
        "# Concatenate the embedding layers and past_deviations_input\n",
        "merged_inputs = Concatenate()([flatten_hour,\n",
        "                                flatten_dow, flatten_month,\n",
        "                              flatten_dom, flatten_year,\n",
        "                               past_regressors_input])\n",
        "\n",
        "quantiles = [0.1, 0.5, 0.9]  # Example quantiles\n",
        "num_models=len(quantiles)\n",
        "predictions = [] # store predicitons\n",
        "\n",
        "for i, q in enumerate(quantiles):\n",
        "    # Add dense layers\n",
        "    hidden_1 = Dense(best_hps.values['units_1'], activation='relu')(merged_inputs)\n",
        "    hidden_2 = Dense(best_hps.values['units_2'], activation='relu')(hidden_1)\n",
        "\n",
        "    # Create separate output neurons for each time step\n",
        "    outputs = []\n",
        "    for i in range(24):\n",
        "        output = Dense(1, activation='linear', name=f'time_step_{i+1}')(hidden_2)\n",
        "        outputs.append(output)\n",
        "\n",
        "    # Create the model\n",
        "    model = keras.Model(inputs=[hour_input,\n",
        "                                dow_input, month_input,\n",
        "                              dom_input, year_input,\n",
        "                               past_regressors_input],\n",
        "                              outputs=outputs)\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=best_hps.values['learning_rate'])\n",
        "    model.compile(loss=lambda y_true, y_pred: quantile_loss(q, y_true, y_pred), optimizer='adam')\n",
        "\n",
        "    # Train the model on the combined training and validation sets\n",
        "    model.fit(x=[trainval_calendar_features[:, 0], trainval_calendar_features[:, 1], trainval_calendar_features[:, 2],\n",
        "                          trainval_calendar_features[:, 3], trainval_calendar_features[:, 4],\n",
        "                          trainval_past_regressors],\n",
        "                        y=[trainval_target[:, i] for i in range(24)],\n",
        "\n",
        "              validation_data=([calendar_features_val[:, 0], calendar_features_val[:, 1], calendar_features_val[:, 2],\n",
        "                       calendar_features_val[:, 3], calendar_features_val[:, 4],\n",
        "                       past_regressors_val],\n",
        "                    [target_variable_val[:, i] for i in range(24)]),\n",
        "\n",
        "                  epochs=10, batch_size=32,\n",
        "                  callbacks=[tensorboard_callback],\n",
        "                  verbose=2)\n",
        "\n",
        "    # Predict on the test data\n",
        "    prediction = model.predict([test_calendar_features[:, 0], test_calendar_features[:, 1],\n",
        "                                    test_calendar_features[:, 2], test_calendar_features[:, 3],\n",
        "                                    test_calendar_features[:, 4],\n",
        "                                    test_past_regressors])\n",
        "    predictions.append(prediction)\n",
        "\n",
        "predictions = np.asarray(predictions)\n",
        "preds = np.squeeze(predictions)\n",
        "preds"
      ],
      "metadata": {
        "id": "hPRjvLXPurxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rescaled_all = pd.DataFrame()\n",
        "\n",
        "for index, element in enumerate(preds_2):\n",
        "    df_predictions = pd.DataFrame()\n",
        "    time_step_predictions = {}\n",
        "    # Access predictions for each time step\n",
        "    for i in range(0, 24):\n",
        "        time_step = i + 1\n",
        "        time_step_prediction = element[i]\n",
        "\n",
        "        # Store the predictions in the dictionary\n",
        "        time_step_predictions[f'time_step_{time_step}_q{str(index+1)}'] = pd.Series(time_step_prediction.flatten())\n",
        "\n",
        "        # Create a DataFrame from the dictionary\n",
        "        df_predictions = pd.concat(time_step_predictions, axis=1)\n",
        "\n",
        "    df_predictions_rescaled = pd.DataFrame(scaler_2.inverse_transform(df_predictions), columns=df_predictions.columns)\n",
        "    rescaled_all = pd.concat([rescaled_all, df_predictions_rescaled], axis=1)\n",
        "\n",
        "test_df = Y_test_df[target]\n",
        "test_df.reset_index(inplace=True)\n",
        "\n",
        "merged_df = pd.concat([test_df, rescaled_all], axis=1)\n",
        "merged_df"
      ],
      "metadata": {
        "id": "qKL1eN99u0wq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine the total number of rows in the DataFrame\n",
        "total_rows = merged_df.shape[0]\n",
        "\n",
        "df_final = pd.DataFrame()\n",
        "# Iterate over the indices in steps of 24\n",
        "for start_index in range(0, total_rows, 24):\n",
        "    # Calculate the end index for each group\n",
        "    end_index = start_index + 24\n",
        "\n",
        "    # Extract the group of rows based on the start and end indices\n",
        "    group_df = merged_df.iloc[start_index:end_index]\n",
        "\n",
        "    date = []\n",
        "    actual = []\n",
        "    pred = []\n",
        "    pred_q1 = []\n",
        "    pred_q2 = []\n",
        "    pred_q3 = []\n",
        "    pred_q4 = []\n",
        "    pred_q5 = []\n",
        "    total_rows_group = group_df.shape[0]\n",
        "    for i in range(0, total_rows_group):\n",
        "        # Calculate the end index for each group\n",
        "        date.append(group_df['ds'].iloc[i])\n",
        "        actual.append(group_df['y_t+0'].iloc[i])\n",
        "        for index, element in enumerate(preds):\n",
        "          if index == 0:\n",
        "            pred_q1.append(group_df[f'time_step_{i+1}_q{str(index+1)[-1]}'].iloc[0])\n",
        "\n",
        "          elif index == 1:\n",
        "            pred_q2.append(group_df[f'time_step_{i+1}_q{str(index+1)[-1]}'].iloc[0])\n",
        "\n",
        "          elif index == 2:\n",
        "            pred_q3.append(group_df[f'time_step_{i+1}_q{str(index+1)[-1]}'].iloc[0])\n",
        "\n",
        "          elif index == 3:\n",
        "            pred_q4.append(group_df[f'time_step_{i+1}_q{str(index+1)[-1]}'].iloc[0])\n",
        "\n",
        "          elif index == 4:\n",
        "            pred_q5.append(group_df[f'time_step_{i+1}_q{str(index+1)[-1]}'].iloc[0])\n",
        "\n",
        "    df_group = pd.DataFrame(list(zip(date, actual, pred_q1, pred_q2, pred_q3, pred_q4, pred_q5 )),\n",
        "                            columns=['date', 'actual', 'pred_q1', 'pred_q2', 'pred_q3', 'pred_q4', 'pred_q5'])\n",
        "\n",
        "\n",
        "    df_final = pd.concat([df_final, df_group], axis=0)\n",
        "\n",
        "df_final"
      ],
      "metadata": {
        "id": "JJmIgXz7u5yy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from neuralforecast.losses.numpy import mae, mape\n",
        "\n",
        "df_final['year_month'] = [x[:7] for x in df_final.date.astype('str')]\n",
        "\n",
        "plt.figure(figsize=(20,5))\n",
        "plt.plot(df_final['date'], df_final['actual'], label='True', c='black')\n",
        "plt.plot(df_final['date'], df_final['pred_q3'], c='steelblue', label='Pred')\n",
        "\n",
        "plt.fill_between(x=df_final['date'],\n",
        "                        y1=df_final['pred_q1'], y2=df_final['pred_q5'],\n",
        "                        alpha=0.6, label='level 90', color='steelblue')\n",
        "\n",
        "plt.fill_between(x=df_final['date'],\n",
        "                        y1=df_final['pred_q2'], y2=df_final['pred_q4'],\n",
        "                        alpha=0.6, label='level 80', color='steelblue')\n",
        "\n",
        "plt.legend()\n",
        "plt.plot()\n",
        "\n",
        "print('MAE: ', mae(df_final['actual'], df_final['pred_q2']))\n",
        "print('MAPE: ', mape(df_final['actual'], df_final['pred_q2']))"
      ],
      "metadata": {
        "id": "v9MDSXZ_u9Hj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def winkler_score(y_true, y_lower, y_upper, alpha=0.1):\n",
        "    delta = y_upper - y_lower\n",
        "\n",
        "    score = np.where((y_true >= y_lower) & (y_true <= y_upper), delta, 0)\n",
        "    score += np.where(y_true < y_lower, delta + (2 / alpha) * (y_lower - y_true), 0)\n",
        "    score += np.where(y_true > y_upper, delta + (2 / alpha) * (y_true - y_upper), 0)\n",
        "\n",
        "    return score.mean()\n",
        "\n",
        "def pi_width(y_lower, y_upper):\n",
        "    delta = y_upper - y_lower\n",
        "    return delta.mean()\n",
        "\n",
        "def coverage_error(y_true, y_lower, y_upper, alpha):\n",
        "    coverage_prob = (y_lower <= y_true) & (y_true <= y_upper)\n",
        "    return np.abs(coverage_prob.mean() - alpha)\n",
        "\n",
        "def pinball(y_true, y_quantile, alpha):\n",
        "    pinball_loss = np.maximum((y_true - y_quantile) * alpha, (y_quantile - y_true) * (1 - alpha))\n",
        "    return np.mean(pinball_loss)\n",
        "\n",
        "def unconditional_coverage_score(y_true, y_lower, y_upper):\n",
        "    coverage_scores = ((y_true >= y_lower) & (y_true <= y_upper)).astype(int)\n",
        "    return np.mean(coverage_scores)"
      ],
      "metadata": {
        "id": "vv3NiR36u_cQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from neuralforecast.losses.numpy import mae, mape, smape\n",
        "from sklearn.metrics import mean_pinball_loss\n",
        "df_final['year_month'] = [x[:7] for x in df_final.index.astype('str')]\n",
        "\n",
        "results_df = pd.DataFrame()\n",
        "\n",
        "for month in df_final['year_month'].drop_duplicates():\n",
        "    df_err = df_final[df_final['year_month']==month]\n",
        "    plt.figure(figsize=(20,5))\n",
        "    plt.plot(df_err.index, df_err['actual'], label='True', c='black')\n",
        "    plt.plot(df_err.index, df_err['pred_q3'], c='steelblue', label='Pred')\n",
        "\n",
        "    plt.fill_between(x=df_err.index,\n",
        "                        y1=df_err['pred_q1'], y2=df_err['pred_q5'],\n",
        "                        alpha=0.3, label='level 90', color='steelblue')\n",
        "    plt.fill_between(x=df_err.index,\n",
        "                        y1=df_err['pred_q2'], y2=df_err['pred_q4'],\n",
        "                        alpha=0.3, label='level 90', color='steelblue')\n",
        "\n",
        "    plt.legend()\n",
        "    plt.plot()\n",
        "\n",
        "    my_results = {'date': pd.to_datetime(month),\n",
        "                  'MAE': round(mae(df_err['actual'], df_err['pred_q3']), 2),\n",
        "                  'sMAPE': round(smape(df_err['actual'], df_err['pred_q3'])*100, 2),\n",
        "                  'Pinball Q90': round(mean_pinball_loss(df_err['actual'], df_err['pred_q4'], alpha=0.9), 2),\n",
        "                  'UC': round(unconditional_coverage_score(df_err['actual'], df_err['pred_q2'], df_err['pred_q4'])*100, 2),\n",
        "                  'Winkler score': round(winkler_score(df_err['actual'], df_err['pred_q2'], df_err['pred_q4'], alpha=0.2), 2),\n",
        "                  'PI width': round(pi_width(df_err['pred_q2'], df_err['pred_q4']), 2)}\n",
        "\n",
        "    results = pd.DataFrame([my_results])\n",
        "    results['sMAPE'] = results['sMAPE'].astype(str) + '%'\n",
        "    results['ACE'] = round((80 - results['UC']), 2).astype(str) + '%'\n",
        "    results['UC'] = results['UC'].astype(str) + '%'\n",
        "    results['Penalty'] = results['Winkler score'] - results['PI width']\n",
        "    results_df = pd.concat([results_df, results], axis=0, ignore_index=True)\n",
        "results_df"
      ],
      "metadata": {
        "id": "RrmY_rN2vCRg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}